"""
Calculate Table 2 metrics: Accuracy, F1 Score, FAR, FRR, and Line Coverage
for unit test quality evaluation.

This script evaluates the quality of unit tests generated by different reward models
(Llama3.1-8B, Llama3.1-70B, CodeRM-8B) when used with Llama3.1-8B as policy model.
"""

import json
import os
import tempfile
import subprocess
import ast
import shutil
from collections import defaultdict
from tqdm import tqdm
import numpy as np
try:
    from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
except ImportError:
    print("Warning: sklearn not installed. Install with: pip install scikit-learn")
    # Fallback implementations
    def accuracy_score(y_true, y_pred):
        return sum(1 for a, b in zip(y_true, y_pred) if a == b) / len(y_true) if len(y_true) > 0 else 0.0
    
    def f1_score(y_true, y_pred):
        from collections import Counter
        cm = confusion_matrix(y_true, y_pred)
        if cm.shape == (2, 2):
            TN, FP, FN, TP = cm.ravel()
            precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0
            recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0
            return 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        return 0.0
    
    def confusion_matrix(y_true, y_pred):
        from collections import Counter
        cm = [[0, 0], [0, 0]]
        for true_val, pred_val in zip(y_true, y_pred):
            cm[true_val][pred_val] += 1
        import numpy as np
        return np.array([[cm[0][0], cm[0][1]], [cm[1][0], cm[1][1]]])


def load_jsonl(filename):
    """Load JSONL file."""
    with open(filename, "r", encoding='utf-8') as f:
        return [json.loads(line) for line in f]


def save_jsonl(filename, dataset):
    """Save data to JSONL file."""
    with open(filename, 'w', encoding='UTF-8') as fp:
        for data in dataset:
            fp.write(json.dumps(data, ensure_ascii=False) + '\n')


def execute_unit_test_with_coverage(solution_code, unit_test_code, timeout=10):
    """
    Execute unit test and calculate line coverage using coverage.py.
    
    Returns:
        dict: {
            'result': 'pass' or 'fail' or 'error',
            'coverage': float (0-1),  # line coverage percentage
            'lines_covered': int,
            'lines_total': int
        }
    """
    import ast
    
    # Count total lines in solution code (excluding empty lines and comments)
    solution_lines = [line.strip() for line in solution_code.split('\n') 
                     if line.strip() and not line.strip().startswith('#')]
    total_solution_lines = len(solution_lines)
    
    # Combine solution and unit test code
    full_code = solution_code + '\n\n' + unit_test_code
    
    # Create temporary directory for coverage measurement
    temp_dir = tempfile.mkdtemp()
    temp_file = os.path.join(temp_dir, 'test_code.py')
    
    try:
        # Write code to temp file
        with open(temp_file, 'w') as f:
            f.write(full_code)
        
        # Extract function name from solution code for coverage measurement
        # We'll measure coverage of the solution function
        function_name = None
        try:
            tree = ast.parse(solution_code)
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    function_name = node.name
                    break
        except:
            pass
        
        # Use coverage.py to measure line coverage
        coverage_result = {'coverage': 0.0, 'lines_covered': 0, 'lines_total': total_solution_lines}
        
        try:
            # Run coverage
            coverage_process = subprocess.run(
                ['coverage', 'run', '--source=.', temp_file],
                capture_output=True,
                text=True,
                timeout=timeout,
                cwd=temp_dir
            )
            
            # Get coverage report
            report_process = subprocess.run(
                ['coverage', 'report', '--format=json'],
                capture_output=True,
                text=True,
                cwd=temp_dir
            )
            
            # Parse JSON coverage report
            if report_process.returncode == 0:
                try:
                    coverage_data = json.loads(report_process.stdout)
                    files = coverage_data.get('files', {})
                    
                    # Find our temp file in coverage data
                    for file_path, file_data in files.items():
                        if 'test_code.py' in file_path or temp_file in file_path:
                            summary = file_data.get('summary', {})
                            total_lines = summary.get('num_statements', 0)
                            covered_lines = summary.get('covered_lines', 0)
                            
                            if total_lines > 0:
                                coverage_pct = covered_lines / total_lines
                                coverage_result = {
                                    'coverage': coverage_pct,
                                    'lines_covered': covered_lines,
                                    'lines_total': total_lines
                                }
                            break
                except json.JSONDecodeError:
                    # Fallback: try to parse text report
                    report_lines = report_process.stdout.split('\n')
                    for line in report_lines:
                        if 'TOTAL' in line.upper():
                            parts = line.split()
                            if len(parts) >= 4:
                                try:
                                    total = int(parts[1])
                                    missed = int(parts[2])
                                    covered = total - missed
                                    coverage_pct = float(parts[3].rstrip('%')) / 100.0
                                    coverage_result = {
                                        'coverage': coverage_pct,
                                        'lines_covered': covered,
                                        'lines_total': total
                                    }
                                except (ValueError, IndexError):
                                    pass
                            break
            
            # Clean up coverage data
            subprocess.run(['coverage', 'erase'], cwd=temp_dir, capture_output=True)
            
        except Exception as e:
            # If coverage calculation fails, use default values
            pass
        
        # Execute test to get result
        try:
            process = subprocess.run(
                ['python', temp_file],
                capture_output=True,
                text=True,
                timeout=timeout
            )
            # Check if test passed (no assertion errors)
            if process.returncode == 0:
                result = 'pass'
            else:
                result = 'fail'
        except subprocess.TimeoutExpired:
            result = 'error'
        except Exception:
            result = 'error'
        
        return {
            'result': result,
            **coverage_result
        }
        
    except Exception as e:
        return {
            'result': 'error',
            'coverage': 0.0,
            'lines_covered': 0,
            'lines_total': total_solution_lines
        }
    finally:
        # Clean up temp directory
        import shutil
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir, ignore_errors=True)


def calculate_metrics_for_individual_tests(benchmark, sol_model, ut_model, sol_num=100, ut_num=100, calculate_coverage=False):
    """
    Calculate metrics for individual unit tests.
    
    For each unit test, we evaluate its classification performance:
    - TP: Correct solution passes the test
    - TN: Incorrect solution fails the test
    - FP: Incorrect solution passes the test (False Acceptance)
    - FN: Correct solution fails the test (False Rejection)
    """
    print(f"Calculating metrics for individual unit tests: {ut_model}")
    
    # Load benchmark data
    benchmark_data = load_jsonl(f'data/benchmark/input_{benchmark}_sol.jsonl')
    
    # Load solution annotations (ground truth)
    if benchmark != 'livecodebench':
        sol_anno = load_jsonl(f'data/result/{benchmark}/sol_{sol_model}_200_anno.jsonl')
        result_key = 'plus_status'
    else:
        sol_anno = load_jsonl(f'data/result/{benchmark}/sol_{sol_model}_100_anno.jsonl')
        result_key = 'result'
    
    # Create solution ground truth mapping
    task_sol_ground_truth = {}
    task_sol_code = {}
    for data in sol_anno:
        task_id = data['task_id']
        for sol_id, sol_data in enumerate(data['solutions']):
            if isinstance(sol_data, dict):
                task_sol_ground_truth[f"{task_id}-{sol_id}"] = sol_data[result_key]
                task_sol_code[f"{task_id}-{sol_id}"] = sol_data['solution']
            else:
                # For livecodebench format
                task_sol_ground_truth[f"{task_id}-{sol_id}"] = sol_data[result_key]
                task_sol_code[f"{task_id}-{sol_id}"] = sol_data
    
    # Load unit test execution results
    ut_result_file = f'output/{benchmark}/{sol_model}_sol_{ut_model}_ut/details/{sol_num}_sol_{ut_num}_ut_result.jsonl'
    if not os.path.exists(ut_result_file):
        raise FileNotFoundError(f"Unit test results not found: {ut_result_file}")
    
    ut_results = load_jsonl(ut_result_file)
    
    # Load unit tests
    ut_dataset = load_jsonl(f'data/result/{benchmark}/ut_{ut_model}_100.jsonl')
    
    # Create mapping: task_id -> ut_id -> unit_test_code
    task_ut_code = {}
    for data in ut_dataset:
        task_id = data['task_id']
        task_ut_code[task_id] = data['unit_tests']
    
    # Create execution result mapping
    task_sol_ut_results = {}
    for data in ut_results:
        key = f"{data['task_id']}-{data['sol_id']}-{data['ut_id']}"
        task_sol_ut_results[key] = data['result']
    
    # Calculate metrics for each unit test
    all_predictions = []  # Predictions: 1 if pass, 0 if fail
    all_labels = []       # Labels: 1 if correct solution, 0 if incorrect
    
    # For line coverage
    coverage_data = []
    
    # Process each task
    for task_data in tqdm(benchmark_data, desc="Processing tasks"):
        task_id = task_data['task_id']
        
        # Get unit tests for this task
        if task_id not in task_ut_code:
            continue
        
        unit_tests = task_ut_code[task_id]
        
        # For each unit test
        for ut_id in range(min(ut_num, len(unit_tests))):
            ut_code = unit_tests[ut_id]
            
            # Collect predictions and labels for this unit test
            ut_predictions = []
            ut_labels = []
            
            # Test on all solutions
            for sol_id in range(min(sol_num, 100)):
                key = f"{task_id}-{sol_id}-{ut_id}"
                sol_key = f"{task_id}-{sol_id}"
                
                if key not in task_sol_ut_results:
                    continue
                
                if sol_key not in task_sol_ground_truth:
                    continue
                
                # Get test result
                test_result = task_sol_ut_results[key]
                ground_truth = task_sol_ground_truth[sol_key]
                
                # Prediction: 1 if pass, 0 if fail/error
                prediction = 1 if test_result == 'pass' else 0
                # Label: 1 if correct solution, 0 if incorrect
                label = 1 if ground_truth == 'pass' else 0
                
                ut_predictions.append(prediction)
                ut_labels.append(label)
                
                # Calculate coverage for this solution-ut pair (optional, can be slow)
                if calculate_coverage and sol_key in task_sol_code:
                    # Sample: only calculate for first few unit tests per solution to speed up
                    if ut_id < 5:  # Calculate coverage for first 5 unit tests per solution
                        sol_code = task_sol_code[sol_key]
                        cov_result = execute_unit_test_with_coverage(sol_code, ut_code)
                        coverage_data.append({
                            'task_id': task_id,
                            'sol_id': sol_id,
                            'ut_id': ut_id,
                            'coverage': cov_result['coverage'],
                            'lines_covered': cov_result['lines_covered'],
                            'lines_total': cov_result['lines_total']
                        })
            
            # Add to global lists
            all_predictions.extend(ut_predictions)
            all_labels.extend(ut_labels)
    
    # Calculate metrics
    if len(all_predictions) == 0 or len(all_labels) == 0:
        print("Warning: No data to calculate metrics")
        return {
            'accuracy': 0.0,
            'f1': 0.0,
            'far': 0.0,
            'frr': 0.0,
            'line_coverage': 0.0
        }
    
    # Accuracy
    accuracy = accuracy_score(all_labels, all_predictions) * 100
    
    # F1 Score
    f1 = f1_score(all_labels, all_predictions) * 100
    
    # Confusion matrix: [TN, FP], [FN, TP]
    cm = confusion_matrix(all_labels, all_predictions)
    if cm.shape == (2, 2):
        TN, FP, FN, TP = cm.ravel()
    else:
        # Handle edge cases
        if len(cm) == 1:
            if all_labels[0] == 0:
                TN, FP, FN, TP = len(all_labels), 0, 0, 0
            else:
                TN, FP, FN, TP = 0, 0, 0, len(all_labels)
        else:
            TN, FP, FN, TP = 0, 0, 0, 0
    
    # False Acceptance Rate (FAR): FP / (FP + TN)
    # Probability of wrong solutions being accepted
    if (FP + TN) > 0:
        far = (FP / (FP + TN)) * 100
    else:
        far = 0.0
    
    # False Rejection Rate (FRR): FN / (FN + TP)
    # Probability of correct solutions being rejected
    if (FN + TP) > 0:
        frr = (FN / (FN + TP)) * 100
    else:
        frr = 0.0
    
    # Line Coverage: average coverage across all solution-ut pairs
    if coverage_data:
        avg_coverage = np.mean([d['coverage'] for d in coverage_data]) * 100
    else:
        avg_coverage = 0.0
    
    return {
        'accuracy': round(accuracy, 2),
        'f1': round(f1, 2),
        'far': round(far, 2),
        'frr': round(frr, 2),
        'line_coverage': round(avg_coverage, 2),
        'coverage_data': coverage_data
    }


def calculate_metrics_for_multiple_tests(benchmark, sol_model, ut_model, sol_num=100, ut_num=100):
    """
    Calculate metrics when using multiple unit tests together.
    
    A solution is considered "accepted" if it passes the majority of unit tests.
    """
    print(f"Calculating metrics for multiple unit tests: {ut_model}")
    
    # Load benchmark data
    benchmark_data = load_jsonl(f'data/benchmark/input_{benchmark}_sol.jsonl')
    
    # Load solution annotations
    if benchmark != 'livecodebench':
        sol_anno = load_jsonl(f'data/result/{benchmark}/sol_{sol_model}_200_anno.jsonl')
        result_key = 'plus_status'
    else:
        sol_anno = load_jsonl(f'data/result/{benchmark}/sol_{sol_model}_100_anno.jsonl')
        result_key = 'result'
    
    # Create solution ground truth mapping
    task_sol_ground_truth = {}
    for data in sol_anno:
        task_id = data['task_id']
        for sol_id, sol_data in enumerate(data['solutions']):
            if isinstance(sol_data, dict):
                task_sol_ground_truth[f"{task_id}-{sol_id}"] = sol_data[result_key]
            else:
                task_sol_ground_truth[f"{task_id}-{sol_id}"] = sol_data[result_key]
    
    # Load unit test execution results
    ut_result_file = f'output/{benchmark}/{sol_model}_sol_{ut_model}_ut/details/{sol_num}_sol_{ut_num}_ut_result.jsonl'
    if not os.path.exists(ut_result_file):
        raise FileNotFoundError(f"Unit test results not found: {ut_result_file}")
    
    ut_results = load_jsonl(ut_result_file)
    
    # Create execution result mapping
    task_sol_ut_results = {}
    for data in ut_results:
        key = f"{data['task_id']}-{data['sol_id']}-{data['ut_id']}"
        task_sol_ut_results[key] = data['result']
    
    # Group results by task_id and sol_id
    task_sol_results = defaultdict(lambda: defaultdict(list))
    for data in ut_results:
        task_id = data['task_id']
        sol_id = data['sol_id']
        ut_id = data['ut_id']
        if ut_id < ut_num:  # Only use first ut_num tests
            result = 1 if data['result'] == 'pass' else 0
            task_sol_results[task_id][sol_id].append(result)
    
    # Calculate predictions: solution passes if majority of tests pass
    all_predictions = []
    all_labels = []
    
    for task_data in tqdm(benchmark_data, desc="Processing tasks"):
        task_id = task_data['task_id']
        
        if task_id not in task_sol_results:
            continue
        
        for sol_id in range(min(sol_num, 100)):
            sol_key = f"{task_id}-{sol_id}"
            
            if sol_key not in task_sol_ground_truth:
                continue
            
            if sol_id not in task_sol_results[task_id]:
                continue
            
            # Get test results for this solution
            test_results = task_sol_results[task_id][sol_id]
            
            if len(test_results) == 0:
                continue
            
            # Prediction: 1 if majority pass, 0 otherwise
            pass_count = sum(test_results)
            prediction = 1 if pass_count > len(test_results) / 2 else 0
            
            # Label: 1 if correct solution, 0 if incorrect
            ground_truth = task_sol_ground_truth[sol_key]
            label = 1 if ground_truth == 'pass' else 0
            
            all_predictions.append(prediction)
            all_labels.append(label)
    
    # Calculate metrics (same as individual tests)
    if len(all_predictions) == 0 or len(all_labels) == 0:
        print("Warning: No data to calculate metrics")
        return {
            'accuracy': 0.0,
            'f1': 0.0,
            'far': 0.0,
            'frr': 0.0,
            'line_coverage': 0.0
        }
    
    accuracy = accuracy_score(all_labels, all_predictions) * 100
    f1 = f1_score(all_labels, all_predictions) * 100
    
    cm = confusion_matrix(all_labels, all_predictions)
    if cm.shape == (2, 2):
        TN, FP, FN, TP = cm.ravel()
    else:
        TN, FP, FN, TP = 0, 0, 0, 0
    
    far = (FP / (FP + TN)) * 100 if (FP + TN) > 0 else 0.0
    frr = (FN / (FN + TP)) * 100 if (FN + TP) > 0 else 0.0
    
    # For multiple tests, line coverage is the average of all individual test coverages
    # We'll calculate this separately if needed
    
    return {
        'accuracy': round(accuracy, 2),
        'f1': round(f1, 2),
        'far': round(far, 2),
        'frr': round(frr, 2),
        'line_coverage': 0.0  # Will be calculated separately if needed
    }


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Calculate Table 2 metrics")
    parser.add_argument('--benchmark', type=str, default='humaneval+', 
                       help='Benchmark name (humaneval+, mbpp+, livecodebench)')
    parser.add_argument('--sol_model', type=str, default='llama3-8b',
                       help='Policy model that generates solutions')
    parser.add_argument('--ut_model', type=str, required=True,
                       help='Reward model that generates unit tests (llama3-8b, llama3-70b, coderm-8b)')
    parser.add_argument('--sol_num', type=int, default=100,
                       help='Number of solutions')
    parser.add_argument('--ut_num', type=int, default=100,
                       help='Number of unit tests')
    parser.add_argument('--mode', type=str, choices=['individual', 'multiple', 'both'], 
                       default='both',
                       help='Calculate metrics for individual tests, multiple tests, or both')
    parser.add_argument('--output_dir', type=str, default='output/table2_results',
                       help='Output directory for results')
    parser.add_argument('--calculate_coverage', action='store_true',
                       help='Calculate line coverage (can be slow, disabled by default)')
    
    args = parser.parse_args()
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    results = {}
    
    if args.mode in ['individual', 'both']:
        print("\n" + "="*60)
        print("Calculating metrics for INDIVIDUAL unit tests")
        print("="*60)
        if args.calculate_coverage:
            print("Note: Line coverage calculation is enabled (this may take a while)")
        individual_results = calculate_metrics_for_individual_tests(
            args.benchmark, args.sol_model, args.ut_model, 
            args.sol_num, args.ut_num, calculate_coverage=args.calculate_coverage
        )
        results['individual'] = individual_results
        
        print(f"\nIndividual Unit Test Results for {args.ut_model}:")
        print(f"  Accuracy: {individual_results['accuracy']:.2f}%")
        print(f"  F1 Score: {individual_results['f1']:.2f}%")
        print(f"  FAR:      {individual_results['far']:.2f}%")
        print(f"  FRR:      {individual_results['frr']:.2f}%")
        print(f"  Line Coverage: {individual_results['line_coverage']:.2f}%")
    
    if args.mode in ['multiple', 'both']:
        print("\n" + "="*60)
        print("Calculating metrics for MULTIPLE unit tests")
        print("="*60)
        multiple_results = calculate_metrics_for_multiple_tests(
            args.benchmark, args.sol_model, args.ut_model,
            args.sol_num, args.ut_num
        )
        results['multiple'] = multiple_results
        
        print(f"\nMultiple Unit Test Results for {args.ut_model}:")
        print(f"  Accuracy: {multiple_results['accuracy']:.2f}%")
        print(f"  F1 Score: {multiple_results['f1']:.2f}%")
        print(f"  FAR:      {multiple_results['far']:.2f}%")
        print(f"  FRR:      {multiple_results['frr']:.2f}%")
    
    # Save results
    output_file = os.path.join(args.output_dir, 
                               f"{args.benchmark}_{args.sol_model}_{args.ut_model}.json")
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nResults saved to: {output_file}")


if __name__ == '__main__':
    main()
