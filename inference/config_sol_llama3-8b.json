{
    "model_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
    "dtype": "auto",
    "max_model_len": 4096,
    "gpu_memory_utilization": 0.8,
    "max_num_seqs": 512,
    "tensor_parallel_size": 1,
    "sampling_params": {
        "n": 100,
        "max_tokens": 2048,
        "top_p": 0.95,
        "temperature": 0.8,
        "stop": null
    },
    "num_gpus": 1,
    "output_dir": "../output/inference/llama3-8b_solutions",
    "messages_file": "../data/benchmark/input_humaneval+_sol.jsonl"
}
